{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8329ce-c442-4226-bc0f-183097bed6f6",
   "metadata": {},
   "source": [
    "## Procedural memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40bcb974-0d02-49ec-9eb7-cf2a820c3ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "%pip install -U langmem langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131c57c5-570e-4bb0-a9d4-0aa00ea89ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "store = InMemoryStore()\n",
    "store.put((\"instructions\",), key=\"agent_instructions\", value={\"prompt\": \"Write good emails.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e7817c-f876-4f70-80ce-1131f74c8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.config import get_store\n",
    "\n",
    "def draft_email(to: str, subject: str, body: str):\n",
    "    \"\"\"Submit an email draft.\"\"\"\n",
    "    return \"Draft saved succesfully.\"\n",
    "\n",
    "def prompt(state):\n",
    "    item = store.get((\"instructions\",), key=\"agent_instructions\")\n",
    "    instructions = item.value[\"prompt\"]\n",
    "    sys_prompt = {\"role\": \"system\", \"content\": f\"## Instructions\\n\\n{instructions}\"}\n",
    "    return [sys_prompt] + state['messages']\n",
    "\n",
    "agent = create_react_agent(\"gpt-3.5-turbo\", prompt=prompt, tools=[draft_email], store=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b3a50b-93c0-4ac8-9de0-4f4058d872fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  draft_email (call_AxcZNDW51cvkLBIqwGxmxTJc)\n",
      " Call ID: call_AxcZNDW51cvkLBIqwGxmxTJc\n",
      "  Args:\n",
      "    to: joe@langchain.dev\n",
      "    subject: Follow-up Meeting Schedule\n",
      "    body: Dear Joe,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a follow-up meeting with you on Thursday at noon to discuss our previous conversation further.\n",
      "\n",
      "Please let me know if this time works for you, and if not, suggest an alternative time that suits your schedule.\n",
      "\n",
      "Looking forward to our discussion.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"user\", \"content\" :\"Draft an email to joe@langchain.dev saying that we want to schedule a followup meeting for thursday at noon.\"}]}\n",
    ")\n",
    "result['messages'][1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159932a-23a1-421f-9119-07b9f4dc5e69",
   "metadata": {},
   "source": [
    "# Update the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa4675d-afff-45f7-8836-da0c132b8caa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'OpenAI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangmem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_prompt_optimizer\n\u001b[0;32m----> 3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_prompt_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm_agent/lib/python3.13/site-packages/langmem/prompts/optimization.py:261\u001b[0m, in \u001b[0;36mcreate_prompt_optimizer\u001b[0;34m(model, kind, config)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a prompt optimizer that improves prompt effectiveness.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mThis function creates an optimizer that can analyze and improve prompts for better\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m        - Uses separation of concerns to extract feedback from more conversational context.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_gradient_prompt_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetaprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_metaprompt_optimizer(model, config)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm_agent/lib/python3.13/site-packages/langmem/prompts/gradient.py:419\u001b[0m, in \u001b[0;36mcreate_gradient_prompt_optimizer\u001b[0;34m(model, config)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_gradient_prompt_optimizer\u001b[39m(\n\u001b[1;32m    414\u001b[0m     model: Union[\u001b[38;5;28mstr\u001b[39m, BaseChatModel], config: Optional[GradientOptimizerConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    415\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GradientPromptOptimizer:\n\u001b[1;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Original factory function that just returns the new class-based optimizer.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGradientPromptOptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm_agent/lib/python3.13/site-packages/langmem/prompts/gradient.py:150\u001b[0m, in \u001b[0;36mGradientPromptOptimizer.__init__\u001b[0;34m(self, model, config)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Decides whether a prompt should be adjusted.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    If warrants_adjustment is True, we incorporate recommended changes.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    If not, we respond 'No recommendations.'\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjust_think_chain \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_extractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritique\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43many\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39many_chain \u001b[38;5;241m=\u001b[39m create_extractor(\n\u001b[1;32m    156\u001b[0m     model,\n\u001b[1;32m    157\u001b[0m     tools\u001b[38;5;241m=\u001b[39m[think, critique, recommend],\n\u001b[1;32m    158\u001b[0m     tool_choice\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_chain \u001b[38;5;241m=\u001b[39m create_extractor(\n\u001b[1;32m    161\u001b[0m     model,\n\u001b[1;32m    162\u001b[0m     tools\u001b[38;5;241m=\u001b[39m[recommend],\n\u001b[1;32m    163\u001b[0m     tool_choice\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecommend\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    164\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm_agent/lib/python3.13/site-packages/trustcall/_base.py:285\u001b[0m, in \u001b[0;36mcreate_extractor\u001b[0;34m(llm, tools, tool_choice, enable_inserts, enable_updates, enable_deletes, existing_schema_policy)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating extractors from a string requires langchain>=0.3.0,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m as well as the provider-specific package\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (like langchain-openai, langchain-anthropic, etc.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please install langchain to continue.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         )\n\u001b[0;32m--> 285\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43minit_chat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m builder \u001b[38;5;241m=\u001b[39m StateGraph(ExtractionState)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_exception\u001b[39m(error: \u001b[38;5;167;01mBaseException\u001b[39;00m, call: ToolCall, schema: Type[BaseModel]):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm_agent/lib/python3.13/site-packages/langchain/chat_models/base.py:325\u001b[0m, in \u001b[0;36minit_chat_model\u001b[0;34m(model, model_provider, configurable_fields, config_prefix, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_prefix\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m has been set but no fields are configurable. Set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`configurable_fields=(...)` to specify the model params that are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m configurable_fields:\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_init_chat_model_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_provider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm_agent/lib/python3.13/site-packages/langchain/chat_models/base.py:348\u001b[0m, in \u001b[0;36m_init_chat_model_helper\u001b[0;34m(model, model_provider, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     _check_pkg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_openai\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m--> 348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    350\u001b[0m     _check_pkg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_anthropic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm_agent/lib/python3.13/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm_agent/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:577\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(\n\u001b[1;32m    574\u001b[0m             proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy, verify\u001b[38;5;241m=\u001b[39mglobal_ssl_context\n\u001b[1;32m    575\u001b[0m         )\n\u001b[1;32m    576\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msync_specific)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'OpenAI'"
     ]
    }
   ],
   "source": [
    "from langmem import create_prompt_optimizer\n",
    "\n",
    "optimizer = create_prompt_optimizer(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80f55a9-f8f1-4b01-adeb-f24152b5c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_prompt = store.get((\"instructions\",), key=\"agent_instructions\").value[\"prompt\"]\n",
    "feedback = {\"request\": \"Always sign off from 'William'; for meeting requests, offer to schedule on Zoom or Google Meet\"}\n",
    "\n",
    "optimizer_result = optimizer.invoke({\"prompt\": current_prompt, \"trajectories\": [(result[\"messages\"], feedback)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357c3086-bad6-46cc-81cf-9b2442940c92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write good emails. Remember to always sign off as 'William' and offer to schedule the meeting on Zoom or Google Meet for meeting requests.\n"
     ]
    }
   ],
   "source": [
    "print(optimizer_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07c09ab8-4649-4603-a760-55a6e77559f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.put((\"instructions\",), key=\"agent_instructions\", value={\"prompt\": optimizer_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38b190a0-a72a-4894-a2ad-eeee9657edc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  draft_email (call_E7ELcSU1VDEWm9KoQu4OmCWR)\n",
      " Call ID: call_E7ELcSU1VDEWm9KoQu4OmCWR\n",
      "  Args:\n",
      "    to: joe@langchain.dev\n",
      "    subject: Follow-up Meeting Schedule\n",
      "    body: Dear Joe,\n",
      "\n",
      "I hope this email finds you well. I wanted to follow up on our previous discussion and schedule a follow-up meeting for Thursday at noon. This meeting will give us the opportunity to discuss further the points we addressed earlier.\n",
      "\n",
      "Please let me know if this time works for you, or suggest an alternative time that would be more convenient. I can schedule the meeting on Zoom or Google Meet, whichever platform you prefer.\n",
      "\n",
      "Looking forward to our meeting and further collaboration.\n",
      "\n",
      "Best regards,\n",
      "William\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"user\", \"content\" :\"Draft an email to joe@langchain.dev saying that we want to schedule a followup meeting for thursday at noon.\"}]}\n",
    ")\n",
    "result['messages'][1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3cbfc93-5d1f-407d-b0a9-4f86d875ceb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  draft_email (call_syOc49cE4D5BI9geRqfPMUa4)\n",
      " Call ID: call_syOc49cE4D5BI9geRqfPMUa4\n",
      "  Args:\n",
      "    to: roger@langchain.dev\n",
      "    subject: Update on Release Timing\n",
      "    body: Hi Roger,\n",
      "\n",
      "I hope this message finds you well. I wanted to inform you that the release has been rescheduled to 4:00 PM today. Please adjust your schedule accordingly.\n",
      "\n",
      "If you have any questions or concerns, feel free to reach out. Thank you for your understanding.\n",
      "\n",
      "Best regards,\n",
      "William\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"user\", \"content\" : \"Let roger@langchain.dev know that the release should be later by 4:00 PM.\"}]}\n",
    ")\n",
    "result['messages'][1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630d1f7-9772-4471-bca0-22558d4978e9",
   "metadata": {},
   "source": [
    "## Multi-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52a85066-00eb-4986-a894-389b07d78da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "%pip install -U langgraph-supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d7a8b09-fe7d-4053-9a73-6a7d0e227404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.config import get_store\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "store.put((\"instructions\",), key=\"email_agent\", value={\"prompt\": \"Write good emails. Repeat your draft content to the user after submitting.\"})\n",
    "store.put((\"instructions\",), key=\"twitter_agent\", value={\"prompt\": \"Write fire tweets. Repeat the tweet content to the user upon submission.\"})\n",
    "\n",
    "## Email agent\n",
    "def draft_email(to: str, subject: str, body: str):\n",
    "    \"\"\"Submit an email draft.\"\"\"\n",
    "    return \"Draft saved succesfully.\"\n",
    "\n",
    "def prompt_email(state):\n",
    "    item = store.get((\"instructions\",), key=\"email_agent\")\n",
    "    instructions = item.value[\"prompt\"]\n",
    "    sys_prompt = {\"role\": \"system\", \"content\": f\"## Instructions\\n\\n{instructions}\"}\n",
    "    return [sys_prompt] + state['messages']\n",
    "\n",
    "email_agent = create_react_agent(\n",
    "    \"gpt-3.5-turbo\", \n",
    "    prompt=prompt_email, \n",
    "    tools=[draft_email], \n",
    "    store=store,\n",
    "    name=\"email_assistant\",\n",
    ")\n",
    "\n",
    "## Tweet\n",
    "\n",
    "def tweet(to: str, subject: str, body: str):\n",
    "    \"\"\"Poast a tweet.\"\"\"\n",
    "    return \"Legendary.\"\n",
    "\n",
    "def prompt_social_media(state):\n",
    "    item = store.get((\"instructions\",), key=\"twitter_agent\")\n",
    "    instructions = item.value[\"prompt\"]\n",
    "    sys_prompt = {\"role\": \"system\", \"content\": f\"## Instructions\\n\\n{instructions}\"}\n",
    "    return [sys_prompt] + state['messages']\n",
    "\n",
    "social_media_agent = create_react_agent(\n",
    "    \"gpt-3.5-turbo\", \n",
    "    prompt=prompt_social_media, \n",
    "    tools=[tweet], \n",
    "    store=store,\n",
    "    name=\"social_media_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76f062c4-2f5f-4a9a-b4dc-a23ec4651b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_supervisor import create_supervisor\n",
    "\n",
    "# Create supervisor workflow\n",
    "workflow = create_supervisor(\n",
    "    [email_agent, social_media_agent],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    prompt=(\n",
    "        \"You are a team supervisor managing email and tweet assistants to help with correspondance.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compile and run\n",
    "app = workflow.compile(store=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff861c5a-d9ff-4905-b902-6b5550801539",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"user\", \"content\" :\"Draft an email to joe@langchain.dev saying that we want to schedule a followup meeting for thursday at noon.\"}]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16857af2-23bf-4a1c-a55b-7ae205decdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: email_assistant\n",
      "\n",
      "I have drafted an email to Joe at joe@langchain.dev regarding scheduling a follow-up meeting for Thursday at noon. Here is the content:\n",
      "\n",
      "Subject: Follow-Up Meeting Schedule\n",
      "\n",
      "Hi Joe,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a follow-up meeting with you for this Thursday at noon. Please let me know if this timing works for you.\n",
      "\n",
      "Looking forward to our discussion.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "\n",
      "Please let me know if you would like me to make any changes or if you are ready to send it.\n"
     ]
    }
   ],
   "source": [
    "result[\"messages\"][3].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d29809be-0c4e-47b8-befe-1c5ef71594bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmem import create_multi_prompt_optimizer\n",
    "\n",
    "feedback = {\"request\": \"Always sign off emails from 'William'; for meeting requests, offer to schedule on Zoom or Google Meet\"}\n",
    "\n",
    "optimizer = create_multi_prompt_optimizer(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6734520b-f0be-428f-b538-c17824d2ec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mPrompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "TypedDict for structured prompt management and optimization.\n",
      "\n",
      "Example:\n",
      "    ```python\n",
      "    from langmem import Prompt\n",
      "\n",
      "    prompt = Prompt(\n",
      "        name=\"extract_entities\",\n",
      "        prompt=\"Extract key entities from the text:\",\n",
      "        update_instructions=\"Make minimal changes, only address where\"\n",
      "        \" errors have occurred after reasoning over why they occur.\",\n",
      "        when_to_update=\"If there seem to be errors in recall of named entities.\",\n",
      "    )\n",
      "    ```\n",
      "\n",
      "The name and prompt fields are required. Optional fields control optimization:\n",
      "- update_instructions: Guidelines for modifying the prompt\n",
      "- when_to_update: Dependencies between prompts during optimization\n",
      "\n",
      "Use in the prompt optimizers.\n",
      "\u001b[0;31mFile:\u001b[0m           /opt/anaconda3/envs/llm_agent/lib/python3.13/site-packages/langmem/prompts/types.py\n",
      "\u001b[0;31mType:\u001b[0m           _TypedDictMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "from langmem import Prompt\n",
    "?Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1813fa94-501b-43a0-8d20-c44ee2c037ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_prompt = store.get((\"instructions\",), key=\"email_agent\").value['prompt']\n",
    "tweet_prompt = store.get((\"instructions\",), key=\"twitter_agent\").value['prompt']\n",
    "\n",
    "email_prompt = {\n",
    "    \"name\": \"email_prompt\",\n",
    "    \"prompt\": email_prompt,\n",
    "    \"when_to_update\": \"Only if feedback is provided indicating email writing performance needs improved.\"\n",
    "}\n",
    "tweet_prompt = {\n",
    "    \"name\": \"tweet_prompt\",\n",
    "    \"prompt\": tweet_prompt,\n",
    "    \"when_to_update\": \"Only if tweet writing generation needs improvement.\"\n",
    "}\n",
    "\n",
    "\n",
    "optimizer_result = optimizer.invoke({\"prompts\": [tweet_prompt, email_prompt], \"trajectories\": [(result[\"messages\"], feedback)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b6a5d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item(namespace=['instructions'], key='email_agent', value={'prompt': \"Write good emails. Repeat your draft content to the user after submitting. Always sign off as 'William' and offer to schedule meetings on Zoom or Google Meet for meeting requests.\"}, created_at='2025-03-04T10:35:30.473418+00:00', updated_at='2025-03-04T10:35:30.473421+00:00')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.get((\"instructions\",), key=\"email_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbc9d37a-23de-4d46-a7c4-219ba84b7625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tweet_prompt',\n",
       "  'prompt': 'Write fire tweets. Repeat the tweet content to the user upon submission.',\n",
       "  'when_to_update': 'Only if tweet writing generation needs improvement.'},\n",
       " {'name': 'email_prompt',\n",
       "  'prompt': \"Write good emails. Repeat your draft content to the user after submitting. Always sign off as 'William' and offer to schedule meetings on Zoom or Google Meet for meeting requests.\",\n",
       "  'when_to_update': 'Only if feedback is provided indicating email writing performance needs improved.'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ef3d1fd-00b4-42a6-b56b-cd9b2d135b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.put((\"instructions\",), key=\"email_agent\", value={\"prompt\": optimizer_result[1]['prompt']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8d66705-a569-43cc-8c45-668cdec62f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"user\", \"content\" :\"Draft an email to joe@langchain.dev saying that we want to schedule a followup meeting for thursday at noon.\"}]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11ae5952-01ce-4374-bd38-c9a22ec44d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: supervisor\n",
      "Tool Calls:\n",
      "  transfer_to_email_assistant (call_SMlIX5zaf9Y4uLVDohrl1yhs)\n",
      " Call ID: call_SMlIX5zaf9Y4uLVDohrl1yhs\n",
      "  Args:\n",
      "    recipient_email: joe@langchain.dev\n",
      "    subject: Follow-up Meeting Schedule\n",
      "    message: Hello Joe,\n",
      "\n",
      "I hope this email finds you well. We would like to schedule a follow-up meeting with you on Thursday at noon. Please let us know if this time works for you.\n",
      "\n",
      "Looking forward to our discussion.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "result[\"messages\"][1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1483f98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Draft an email to joe@langchain.dev saying that we want to schedule a followup meeting for thursday at noon.', additional_kwargs={}, response_metadata={}, id='df8ce96d-71d4-4a74-b7fe-dc41647975ca'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_SMlIX5zaf9Y4uLVDohrl1yhs', 'function': {'arguments': '{\"recipient_email\":\"joe@langchain.dev\",\"subject\":\"Follow-up Meeting Schedule\",\"message\":\"Hello Joe,\\\\n\\\\nI hope this email finds you well. We would like to schedule a follow-up meeting with you on Thursday at noon. Please let us know if this time works for you.\\\\n\\\\nLooking forward to our discussion.\\\\n\\\\nBest regards,\\\\n[Your Name]\"}', 'name': 'transfer_to_email_assistant'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 100, 'total_tokens': 192, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, name='supervisor', id='run-f1690be6-7623-4536-acba-957a42ec766f-0', tool_calls=[{'name': 'transfer_to_email_assistant', 'args': {'recipient_email': 'joe@langchain.dev', 'subject': 'Follow-up Meeting Schedule', 'message': 'Hello Joe,\\n\\nI hope this email finds you well. We would like to schedule a follow-up meeting with you on Thursday at noon. Please let us know if this time works for you.\\n\\nLooking forward to our discussion.\\n\\nBest regards,\\n[Your Name]'}, 'id': 'call_SMlIX5zaf9Y4uLVDohrl1yhs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 100, 'output_tokens': 92, 'total_tokens': 192, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='Successfully transferred to email_assistant', name='transfer_to_email_assistant', id='c5707855-7e01-4ec5-8389-dbed7b20cdce', tool_call_id='call_SMlIX5zaf9Y4uLVDohrl1yhs'),\n",
       "  AIMessage(content='I have drafted an email to Joe requesting a follow-up meeting on Thursday at noon. Would you like me to send it?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 225, 'total_tokens': 252, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, name='email_assistant', id='run-19ddf156-d490-401e-81f0-fc538898d49d-0', usage_metadata={'input_tokens': 225, 'output_tokens': 27, 'total_tokens': 252, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  AIMessage(content='Transferring back to supervisor', additional_kwargs={}, response_metadata={}, name='email_assistant', id='ef41e809-67b8-42ad-94f7-74c4b689482c', tool_calls=[{'name': 'transfer_back_to_supervisor', 'args': {}, 'id': '2266622c-ca16-4df0-acaf-cf6a08ebba9b', 'type': 'tool_call'}]),\n",
       "  ToolMessage(content='Successfully transferred back to supervisor', name='transfer_back_to_supervisor', id='1ea1a0a8-2565-4bb5-acf4-aaa32f495723', tool_call_id='2266622c-ca16-4df0-acaf-cf6a08ebba9b'),\n",
       "  AIMessage(content='The email draft is ready for your review and approval before sending.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 291, 'total_tokens': 306, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, name='supervisor', id='run-dbc5b614-392f-482b-90d4-c546e2fc3ed3-0', usage_metadata={'input_tokens': 291, 'output_tokens': 15, 'total_tokens': 306, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12674f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
